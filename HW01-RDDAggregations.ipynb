{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Instructions:\n",
    " - For all cells marked with <font color=\"red\">(CODE Needed)</font> below, replace `#CODE_HERE` with your solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Spark Context (No CODE Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-c24a73648561>:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c24a73648561>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m#.set(\"spark.driver.cores\",4) \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#.set(\"spark.executor.memory\",\"2g\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-c24a73648561>:5 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "myconf = SparkConf() \\\n",
    "        .setMaster(\"local[*]\") \n",
    "        #.set(\"spark.driver.cores\",4) \\\n",
    "        #.set(\"spark.executor.memory\",\"2g\") \n",
    "sc = SparkContext(conf=myconf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load some printing functions (No CODE Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are some printing helper functions we will use to make the output more clear\n",
    "from pprint import pprint\n",
    "def title(s):\n",
    "    pprint(\"---- %s -----\" %s)    \n",
    "    \n",
    "def see(s, v):\n",
    "    pprint(\"---- %s -----\" %s)\n",
    "    pprint(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load each file in an RDD and add the year to the data (No CODE Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- year:2010 -----'\n",
      "'../data/flight-data/csv/2010*'\n",
      "'---- rdd count for year:2010 -----'\n",
      "253\n",
      "'---- rdd elements for year:2010 -----'\n",
      "(['United States', 'Romania', '1'], 2010)\n",
      "(['United States', 'Ireland', '264'], 2010)\n",
      "(['United States', 'India', '69'], 2010)\n",
      "(['Egypt', 'United States', '24'], 2010)\n",
      "(['Equatorial Guinea', 'United States', '1'], 2010)\n",
      "'---- year:2011 -----'\n",
      "'../data/flight-data/csv/2011*'\n",
      "'---- rdd count for year:2011 -----'\n",
      "253\n",
      "'---- rdd elements for year:2011 -----'\n",
      "(['United States', 'Saint Martin', '2'], 2011)\n",
      "(['United States', 'Guinea', '2'], 2011)\n",
      "(['United States', 'Croatia', '1'], 2011)\n",
      "(['United States', 'Romania', '3'], 2011)\n",
      "(['United States', 'Ireland', '268'], 2011)\n",
      "'---- year:2012 -----'\n",
      "'../data/flight-data/csv/2012*'\n",
      "'---- rdd count for year:2012 -----'\n",
      "243\n",
      "'---- rdd elements for year:2012 -----'\n",
      "(['United States', 'Croatia', '1'], 2012)\n",
      "(['United States', 'Ireland', '252'], 2012)\n",
      "(['Egypt', 'United States', '13'], 2012)\n",
      "(['United States', 'India', '62'], 2012)\n",
      "(['United States', 'Singapore', '25'], 2012)\n",
      "'---- year:2013 -----'\n",
      "'../data/flight-data/csv/2013*'\n",
      "'---- rdd count for year:2013 -----'\n",
      "246\n",
      "'---- rdd elements for year:2013 -----'\n",
      "(['United States', 'Romania', '12'], 2013)\n",
      "(['United States', 'Croatia', '1'], 2013)\n",
      "(['United States', 'Ireland', '266'], 2013)\n",
      "(['Egypt', 'United States', '13'], 2013)\n",
      "(['United States', 'India', '60'], 2013)\n",
      "'---- year:2014 -----'\n",
      "'../data/flight-data/csv/2014*'\n",
      "'---- rdd count for year:2014 -----'\n",
      "239\n",
      "'---- rdd elements for year:2014 -----'\n",
      "(['United States', 'Saint Martin', '1'], 2014)\n",
      "(['United States', 'Romania', '12'], 2014)\n",
      "(['United States', 'Croatia', '2'], 2014)\n",
      "(['United States', 'Ireland', '291'], 2014)\n",
      "(['United States', 'India', '62'], 2014)\n",
      "'---- year:2015 -----'\n",
      "'../data/flight-data/csv/2015*'\n",
      "'---- rdd count for year:2015 -----'\n",
      "254\n",
      "'---- rdd elements for year:2015 -----'\n",
      "(['United States', 'Romania', '15'], 2015)\n",
      "(['United States', 'Croatia', '1'], 2015)\n",
      "(['United States', 'Ireland', '344'], 2015)\n",
      "(['Egypt', 'United States', '15'], 2015)\n",
      "(['United States', 'India', '62'], 2015)\n",
      "'---- array of RDDs -----'\n",
      "[PythonRDD[101] at RDD at PythonRDD.scala:48,\n",
      " PythonRDD[102] at RDD at PythonRDD.scala:48,\n",
      " PythonRDD[103] at RDD at PythonRDD.scala:48,\n",
      " PythonRDD[104] at RDD at PythonRDD.scala:48,\n",
      " PythonRDD[105] at RDD at PythonRDD.scala:48,\n",
      " PythonRDD[106] at RDD at PythonRDD.scala:48]\n"
     ]
    }
   ],
   "source": [
    "def createOneYearRdd(year):\n",
    "    file = \"../data/flight-data/csv/%s\"%year+\"*\"\n",
    "    see(\"year:\"+str(year), file)\n",
    "    fileRDD =sc.textFile(file)\n",
    "    linesRDD = fileRDD.filter(lambda x:  not x.startswith(\"DEST_\")) \\\n",
    "              .filter(lambda line: line.count(',')==2)\n",
    "    arrRDD = linesRDD.map(lambda line: (line.split(\",\"), year))\n",
    "    see(\"rdd count for year:\"+str(year), arrRDD.count())\n",
    "    title(\"rdd elements for year:\"+str(year))\n",
    "    for arr in (arrRDD.take(5)):\n",
    "        print(arr)\n",
    "    return arrRDD\n",
    "\n",
    "separateRDDs  = list(map(createOneYearRdd, range(2010,2016)))\n",
    "                 \n",
    "see(\"array of RDDs\", separateRDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Merge all RDDs into one Rdd  (No CODE Needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- allYearsRDD count -----'\n",
      "1488\n",
      "'---- allYearsRDD sample -----'\n",
      "[(['United States', 'Paraguay', '14'], 2014),\n",
      " (['Senegal', 'United States', '29'], 2011),\n",
      " (['United States', 'Samoa', '28'], 2010),\n",
      " (['United States', 'Thailand', '4'], 2015),\n",
      " (['United States', 'Afghanistan', '5'], 2012),\n",
      " (['United States', 'Belgium', '355'], 2010),\n",
      " (['United States', 'Sweden', '68'], 2011),\n",
      " (['United States', 'Sweden', '119'], 2015),\n",
      " (['Czech Republic', 'United States', '13'], 2015),\n",
      " (['Namibia', 'United States', '1'], 2011)]\n"
     ]
    }
   ],
   "source": [
    "allYearsRdd  =sc.union(separateRDDs)\n",
    "see(\"allYearsRDD count\", allYearsRdd.count())\n",
    "see(\"allYearsRDD sample\", allYearsRdd.takeSample(False, 10, 17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compute an RDD containing all flights which departed from  the US  <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `filter` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- fromUSRdd count -----'\n",
      "765\n",
      "'---- fromUSRdd take(10) -----'\n",
      "[(['Egypt', 'United States', '24'], 2010),\n",
      " (['Equatorial Guinea', 'United States', '1'], 2010),\n",
      " (['Costa Rica', 'United States', '477'], 2010),\n",
      " (['Senegal', 'United States', '29'], 2010),\n",
      " (['Guyana', 'United States', '17'], 2010),\n",
      " (['Malta', 'United States', '1'], 2010),\n",
      " (['Bolivia', 'United States', '46'], 2010),\n",
      " (['Anguilla', 'United States', '21'], 2010),\n",
      " (['Turks and Caicos Islands', 'United States', '136'], 2010),\n",
      " (['Saint Vincent and the Grenadines', 'United States', '1'], 2010)]\n"
     ]
    }
   ],
   "source": [
    "fromUSRdd = allYearsRdd.filter(lambda arr:arr[0][1]=='United States')\n",
    "\n",
    "see(\"fromUSRdd count\",fromUSRdd.count())\n",
    "\n",
    "see(\"fromUSRdd take(10)\",fromUSRdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectated output for step 5.\n",
    "\n",
    "<pre>\n",
    "'---- fromUSRdd count -----'\n",
    "765\n",
    "'---- fromUSRdd take(10) -----'\n",
    "[(['Egypt', 'United States', '24'], 2010),\n",
    " (['Equatorial Guinea', 'United States', '1'], 2010),\n",
    " (['Costa Rica', 'United States', '477'], 2010),\n",
    " (['Senegal', 'United States', '29'], 2010),\n",
    " (['Guyana', 'United States', '17'], 2010),\n",
    " (['Malta', 'United States', '1'], 2010),\n",
    " (['Bolivia', 'United States', '46'], 2010),\n",
    " (['Anguilla', 'United States', '21'], 2010),\n",
    " (['Turks and Caicos Islands', 'United States', '136'], 2010),\n",
    " (['Saint Vincent and the Grenadines', 'United States', '1'], 2010)]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create an RDD with a composite key (destination, year) and a value n which is the corresponding number of flights  as an integer <font color=\"red\">(CODE Needed)</font>\n",
    "- Use the `int()` function to convert a string to an integer\n",
    "- Hint: make 1 `map` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- compositeKeyRdd count -----'\n",
      "765\n",
      "'---- compositeKeyRdd take(10) -----'\n",
      "[(('Egypt', 2010), 24),\n",
      " (('Equatorial Guinea', 2010), 1),\n",
      " (('Costa Rica', 2010), 477),\n",
      " (('Senegal', 2010), 29),\n",
      " (('Guyana', 2010), 17),\n",
      " (('Malta', 2010), 1),\n",
      " (('Bolivia', 2010), 46),\n",
      " (('Anguilla', 2010), 21),\n",
      " (('Turks and Caicos Islands', 2010), 136),\n",
      " (('Saint Vincent and the Grenadines', 2010), 1)]\n"
     ]
    }
   ],
   "source": [
    "compositeKeyRdd = fromUSRdd.map(lambda arr:((arr[0][0],arr[1]),int(arr[0][2])))\n",
    "\n",
    "see(\"compositeKeyRdd count\",compositeKeyRdd.count())\n",
    "\n",
    "see(\"compositeKeyRdd take(10)\",compositeKeyRdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step 6\n",
    "<pre>\n",
    "'---- compositeKeyRdd count -----'\n",
    "765\n",
    "'---- compositeKeyRdd take(10) -----'\n",
    "[(('Egypt', 2010), 24),\n",
    " (('Equatorial Guinea', 2010), 1),\n",
    " (('Costa Rica', 2010), 477),\n",
    " (('Senegal', 2010), 29),\n",
    " (('Guyana', 2010), 17),\n",
    " (('Malta', 2010), 1),\n",
    " (('Bolivia', 2010), 46),\n",
    " (('Anguilla', 2010), 21),\n",
    " (('Turks and Caicos Islands', 2010), 136),\n",
    " (('Saint Vincent and the Grenadines', 2010), 1)]\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Sum all the flights for the same destination and the same year <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `reduceByKey` transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- groupedRdd count -----'\n",
      "765\n",
      "'---- groupedRdd take(10) -----'\n",
      "[(('Suriname', 2010), 12),\n",
      " (('Thailand', 2010), 16),\n",
      " (('Ethiopia', 2010), 12),\n",
      " (('Barbados', 2010), 130),\n",
      " (('Cyprus', 2010), 2),\n",
      " (('Fiji', 2010), 53),\n",
      " (('Federated States of Micronesia', 2010), 46),\n",
      " (('French Guiana', 2010), 4),\n",
      " (('Netherlands', 2010), 586),\n",
      " (('China', 2010), 448)]\n"
     ]
    }
   ],
   "source": [
    "groupedRdd = compositeKeyRdd.reduceByKey(lambda x,y:x+y)\n",
    "\n",
    "see(\"groupedRdd count\",groupedRdd.count())\n",
    "\n",
    "see(\"groupedRdd take(10)\",groupedRdd.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step  7 \n",
    "<pre>\n",
    "'---- groupedRdd count -----'\n",
    "765\n",
    "'---- groupedRdd take(10) -----'\n",
    "[(('Ethiopia', 2010), 12),\n",
    " (('Fiji', 2010), 53),\n",
    " (('Federated States of Micronesia', 2010), 46),\n",
    " (('French Guiana', 2010), 4),\n",
    " (('Netherlands', 2010), 586),\n",
    " (('Denmark', 2010), 98),\n",
    " (('Belgium', 2010), 408),\n",
    " (('Guatemala', 2010), 386),\n",
    " (('Vietnam', 2010), 1),\n",
    " (('South Korea', 2010), 683)]\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sort the summed data by the highest sum <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `map` transformation, 1 `sortByKey(ascending=False)`, then 1 `map`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- sortedByCountRdd count -----'\n",
      "765\n",
      "'---- sortedByCountRdd take(10) -----'\n",
      "[(('United States', 2015), 370002),\n",
      " (('United States', 2014), 358354),\n",
      " (('United States', 2011), 352742),\n",
      " (('United States', 2010), 348113),\n",
      " (('United States', 2012), 347452),\n",
      " (('United States', 2013), 343132),\n",
      " (('Canada', 2011), 8514),\n",
      " (('Canada', 2015), 8399),\n",
      " (('Canada', 2010), 8271),\n",
      " (('Canada', 2012), 8034)]\n"
     ]
    }
   ],
   "source": [
    "sortedByCountRdd = groupedRdd.map(lambda arr:(arr[1],arr[0])).sortByKey(ascending=False).map(lambda arr:(arr[1],arr[0]))\n",
    "\n",
    "see(\"sortedByCountRdd count\",sortedByCountRdd.count())\n",
    "\n",
    "see(\"sortedByCountRdd take(10)\",sortedByCountRdd.take(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step  8\n",
    "<pre>\n",
    "'---- sortedByCountRdd count -----'\n",
    "765\n",
    "'---- sortedByCountRdd take(10) -----'\n",
    "[(('United States', 2015), 370002),\n",
    " (('United States', 2014), 358354),\n",
    " (('United States', 2011), 352742),\n",
    " (('United States', 2010), 348113),\n",
    " (('United States', 2012), 347452),\n",
    " (('United States', 2013), 343132),\n",
    " (('Canada', 2011), 8514),\n",
    " (('Canada', 2015), 8399),\n",
    " (('Canada', 2010), 8271),\n",
    " (('Canada', 2012), 8034)]\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Compute the Sum of all Trips <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `map` , 1 `sum`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- sumAllTrips -----'\n",
      "2352144\n"
     ]
    }
   ],
   "source": [
    "sumAllTrips = sortedByCountRdd.map(lambda arr: int(arr[1])).reduce(lambda x,y: x+y)\n",
    "see(\"sumAllTrips\", sumAllTrips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step  9\n",
    "<pre>\n",
    "'---- sumAllTrips -----'\n",
    "2352144\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Compute the Sum of Each Year's Trip <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `map` , 1 `reduceByKey`, 1 `collectAsMap`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- TPY -----'\n",
      "{2010: 385434,\n",
      " 2011: 390613,\n",
      " 2012: 385244,\n",
      " 2013: 380985,\n",
      " 2014: 397960,\n",
      " 2015: 411908}\n"
     ]
    }
   ],
   "source": [
    "TPY = groupedRdd.map(lambda arr:(arr[0][1],int(arr[1]))).reduceByKey(lambda x,y:x+y).collectAsMap()\n",
    "\n",
    "see(\"TPY\", TPY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step  10\n",
    "<pre>\n",
    "'---- TPY -----'\n",
    "{2010: 385434,\n",
    " 2011: 390613,\n",
    " 2012: 385244,\n",
    " 2013: 380985,\n",
    " 2014: 397960,\n",
    " 2015: 411908}\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Use the above sums to compute percent Per Year, percent from Total for each country, year pair <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `map`\n",
    "- use the function `percent`\n",
    "- use TPY as a lookup table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- stats count -----'\n",
      "765\n",
      "'---- stats take(10) -----'\n",
      "[((('United States', 2015), 370002), 89.8263689950183, 15.730414464420546),\n",
      " ((('United States', 2014), 358354), 90.04774349180822, 15.235206688025903),\n",
      " ((('United States', 2011), 352742), 90.30472616118767, 14.99661585345115),\n",
      " ((('United States', 2010), 348113), 90.31714898011073, 14.799816677890469),\n",
      " ((('United States', 2012), 347452), 90.19011327885704, 14.771714656925766),\n",
      " ((('United States', 2013), 343132), 90.06443823247635, 14.588052432164018),\n",
      " ((('Canada', 2011), 8514), 2.1796509588774566, 0.36196763463461423),\n",
      " ((('Canada', 2015), 8399), 2.039047554308244, 0.3570784781884102),\n",
      " ((('Canada', 2010), 8271), 2.145892682015598, 0.3516366344917658),\n",
      " ((('Canada', 2012), 8034), 2.0854315706409445, 0.3415607207721976)]\n"
     ]
    }
   ],
   "source": [
    "def percent(number, total):\n",
    "    return 100.0* number/float(total)\n",
    "\n",
    "stats = sortedByCountRdd.map(lambda arr:(arr,percent(arr[1],TPY[arr[0][1]]),percent(arr[1],sumAllTrips)))\n",
    "                                                    \n",
    "\n",
    "see(\"stats count\",stats.count())\n",
    "\n",
    "see(\"stats take(10)\",stats.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step  11\n",
    "<pre>\n",
    "'---- stats count -----'\n",
    "765\n",
    "'---- stats take(10) -----'\n",
    "[(('United States', 2015), 370002, 89.8263689950183, 15.730414464420546),\n",
    " (('United States', 2014), 358354, 90.04774349180822, 15.235206688025903),\n",
    " (('United States', 2011), 352742, 90.30472616118767, 14.99661585345115),\n",
    " (('United States', 2010), 348113, 90.31714898011073, 14.799816677890469),\n",
    " (('United States', 2012), 347452, 90.19011327885704, 14.771714656925766),\n",
    " (('United States', 2013), 343132, 90.06443823247635, 14.588052432164018),\n",
    " (('Canada', 2011), 8514, 2.1796509588774566, 0.36196763463461423),\n",
    " (('Canada', 2015), 8399, 2.039047554308244, 0.3570784781884102),\n",
    " (('Canada', 2010), 8271, 2.145892682015598, 0.3516366344917658),\n",
    " (('Canada', 2012), 8034, 2.0854315706409445, 0.3415607207721976)]\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Transform the above table into the format below <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `map`, 1 `groupByKey`, 1 `mapValues` \n",
    "- you will also need the python functions `list`, `sorted`, `dict`\n",
    "- use TPY as a lookup table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 431.0 failed 1 times, most recent failure: Lost task 0.0 in stage 431.0 (TID 486, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1926, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-80-41654f9f0c19>\", line 1, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1926, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-80-41654f9f0c19>\", line 1, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-41654f9f0c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreTable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preTable count\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preTable take(5)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpreTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msortByKey\u001b[0;34m(self, ascending, numPartitions, keyfunc)\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# the key-space into bins such that the bins have roughly the same\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0;31m# number of (key, value) pairs falling into them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mrddSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrddSize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# empty RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m         \"\"\"\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m         \u001b[0;36m6.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \"\"\"\n\u001b[0;32m-> 1032\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mfold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m    904\u001b[0m         \u001b[0;31m# zeroValue provided to each partition is unique from the one provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;31m# to the final reduce call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 906\u001b[0;31m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    907\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeroValue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \"\"\"\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 431.0 failed 1 times, most recent failure: Lost task 0.0 in stage 431.0 (TID 486, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1926, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-80-41654f9f0c19>\", line 1, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 177, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 2423, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 346, in func\n    return f(iterator)\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1926, in combine\n    merger.mergeValues(iterator)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-80-41654f9f0c19>\", line 1, in <lambda>\nIndexError: tuple index out of range\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "preTable = stats.map(lambda arr: (arr[0][0][0],(arr[0][0][1],(arr[0][0][2],arr[1],arr[2])))).groupByKey().mapValues(lambda lst:dict(lst)).sortByKey()\n",
    "\n",
    "see(\"preTable count\",preTable.count())\n",
    "\n",
    "see(\"preTable take(5)\",preTable.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step  12\n",
    "<pre>\n",
    "'---- preTable count -----'\n",
    "165\n",
    "'---- preTable take(10) -----'\n",
    "[('Afghanistan',\n",
    "  {2010: (11, 0.002853925704530477, 0.0004676584426803801),\n",
    "   2011: (8, 0.0020480629164927946, 0.0003401152310402764),\n",
    "   2012: (5, 0.0012978787469759426, 0.00021257201940017278)}),\n",
    " ('Algeria',\n",
    "  {2013: (2, 0.0005249550507237817, 8.50288077600691e-05),\n",
    "   2014: (9, 0.002261533822494723, 0.000382629634920311),\n",
    "   2015: (4, 0.0009710906318886742, 0.0001700576155201382)}),\n",
    " ('Angola',\n",
    "  {2010: (14, 0.0036322690784933347, 0.0005952016543204838),\n",
    "   2011: (13, 0.0033281022393007913, 0.0005526872504404493),\n",
    "   2012: (12, 0.003114908992742262, 0.0005101728465604147),\n",
    "   2013: (12, 0.0031497303043426907, 0.0005101728465604147),\n",
    "   2014: (13, 0.003266659965825711, 0.0005526872504404493),\n",
    "   2015: (15, 0.003641589869582528, 0.0006377160582005184)}),\n",
    " ('Anguilla',\n",
    "  {2010: (21, 0.0054484036177400025, 0.0008928024814807257),\n",
    "   2011: (21, 0.005376165155793586, 0.0008928024814807257),\n",
    "   2012: (19, 0.004931939238508582, 0.0008077736737206565),\n",
    "   2013: (22, 0.0057745055579616, 0.0009353168853607602),\n",
    "   2014: (34, 0.008543572218313398, 0.0014454897319211748),\n",
    "   2015: (41, 0.00995367897685891, 0.0017430905590814169)}),\n",
    " ('Antigua and Barbuda',\n",
    "  {2010: (123, 0.03191207833247715, 0.00522927167724425),\n",
    "   2011: (146, 0.0373771482259935, 0.006207102966485045),\n",
    "   2012: (145, 0.03763848366230233, 0.006164588562605011),\n",
    "   2013: (123, 0.03228473561951258, 0.00522927167724425),\n",
    "   2014: (115, 0.028897376620765906, 0.0048891564462039735),\n",
    "   2015: (126, 0.030589354904493236, 0.005356814888884354)})]\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Transform the above table into the format below <font color=\"red\">(CODE Needed)</font>\n",
    "- Hint: make 1 `map`, you will need more than one line in the map, so create a mapping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'---- Table count -----'\n",
      "165\n",
      "'---- Table take(3) -----'\n",
      "[{'Country': 'Afghanistan',\n",
      "  'PcntTotal-2010': 0.0004676584426803801,\n",
      "  'PcntTotal-2011': 0.0003401152310402764,\n",
      "  'PcntTotal-2012': 0.00021257201940017278,\n",
      "  'PcntYear-2010': 0.002853925704530477,\n",
      "  'PcntYear-2011': 0.0020480629164927946,\n",
      "  'PcntYear-2012': 0.0012978787469759426},\n",
      " {'Country': 'Algeria',\n",
      "  'PcntTotal-2013': 8.50288077600691e-05,\n",
      "  'PcntTotal-2014': 0.000382629634920311,\n",
      "  'PcntTotal-2015': 0.0001700576155201382,\n",
      "  'PcntYear-2013': 0.0005249550507237817,\n",
      "  'PcntYear-2014': 0.002261533822494723,\n",
      "  'PcntYear-2015': 0.0009710906318886742},\n",
      " {'Country': 'Angola',\n",
      "  'PcntTotal-2010': 0.0005952016543204838,\n",
      "  'PcntTotal-2011': 0.0005526872504404493,\n",
      "  'PcntTotal-2012': 0.0005101728465604147,\n",
      "  'PcntTotal-2013': 0.0005101728465604147,\n",
      "  'PcntTotal-2014': 0.0005526872504404493,\n",
      "  'PcntTotal-2015': 0.0006377160582005184,\n",
      "  'PcntYear-2010': 0.0036322690784933347,\n",
      "  'PcntYear-2011': 0.0033281022393007913,\n",
      "  'PcntYear-2012': 0.003114908992742262,\n",
      "  'PcntYear-2013': 0.0031497303043426907,\n",
      "  'PcntYear-2014': 0.003266659965825711,\n",
      "  'PcntYear-2015': 0.003641589869582528}]\n"
     ]
    }
   ],
   "source": [
    "def func(s):\n",
    "    o={}\n",
    "    o['Country']=s[0]\n",
    "\n",
    "    for year in list(s[1].keys()):\n",
    "        lh = (\"PcntTotal-%s\")%year\n",
    "        rh = s[1][year][1]\n",
    "        o[lh] = rh \n",
    "   \n",
    "    for year in list(s[1].keys()):\n",
    "        lh = (\"PcntYear-%s\")%year\n",
    "        rh = s[1][year][0]\n",
    "        o[lh] = rh\n",
    "    \n",
    "    return(o)\n",
    "\n",
    "Table = preTable.map(lambda s: func(s))\n",
    "\n",
    "see(\"Table count\",Table.count())\n",
    "\n",
    "see(\"Table take(3)\",Table.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exepected output for step  13\n",
    "<pre>\n",
    "'---- Table count -----'\n",
    "165\n",
    "'---- Table take(3) -----'\n",
    "[{'Country': 'Afghanistan',\n",
    "  'PcntTotal-2010': 0.0004676584426803801,\n",
    "  'PcntTotal-2011': 0.0003401152310402764,\n",
    "  'PcntTotal-2012': 0.00021257201940017278,\n",
    "  'PcntYear-2010': 0.002853925704530477,\n",
    "  'PcntYear-2011': 0.0020480629164927946,\n",
    "  'PcntYear-2012': 0.0012978787469759426,\n",
    "  'Tot-2010': 11,\n",
    "  'Tot-2011': 8,\n",
    "  'Tot-2012': 5},\n",
    " {'Country': 'Algeria',\n",
    "  'PcntTotal-2013': 8.50288077600691e-05,\n",
    "  'PcntTotal-2014': 0.000382629634920311,\n",
    "  'PcntTotal-2015': 0.0001700576155201382,\n",
    "  'PcntYear-2013': 0.0005249550507237817,\n",
    "  'PcntYear-2014': 0.002261533822494723,\n",
    "  'PcntYear-2015': 0.0009710906318886742,\n",
    "  'Tot-2013': 2,\n",
    "  'Tot-2014': 9,\n",
    "  'Tot-2015': 4},\n",
    " {'Country': 'Angola',\n",
    "  'PcntTotal-2010': 0.0005952016543204838,\n",
    "  'PcntTotal-2011': 0.0005526872504404493,\n",
    "  'PcntTotal-2012': 0.0005101728465604147,\n",
    "  'PcntTotal-2013': 0.0005101728465604147,\n",
    "  'PcntTotal-2014': 0.0005526872504404493,\n",
    "  'PcntTotal-2015': 0.0006377160582005184,\n",
    "  'PcntYear-2010': 0.0036322690784933347,\n",
    "  'PcntYear-2011': 0.0033281022393007913,\n",
    "  'PcntYear-2012': 0.003114908992742262,\n",
    "  'PcntYear-2013': 0.0031497303043426907,\n",
    "  'PcntYear-2014': 0.003266659965825711,\n",
    "  'PcntYear-2015': 0.003641589869582528,\n",
    "  'Tot-2010': 14,\n",
    "  'Tot-2011': 13,\n",
    "  'Tot-2012': 12,\n",
    "  'Tot-2013': 12,\n",
    "  'Tot-2014': 13,\n",
    "  'Tot-2015': 15}]\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. Optional: Convert the data to a Pandas Dataframe <font color=\"green\">(No CODE Needed)</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Algeria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angola</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anguilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Antigua and Barbuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Argentina</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Aruba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Azerbaijan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bahrain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Barbados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Belarus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Belgium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Belize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Bermuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Bolivia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>British Virgin Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brunei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Bulgaria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Burkina Faso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Burundi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Cameroon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Cape Verde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Cayman Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Chad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Chile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>China</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Sint Maarten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Slovakia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Solomon Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>South Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>South Korea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Spain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Suriname</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Sweden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Switzerland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Tanzania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Thailand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>The Bahamas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>The Gambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Togo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Tunisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Turkey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Turks and Caicos Islands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Uganda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Ukraine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>United Arab Emirates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Uruguay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Venezuela</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Yemen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Zambia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>Zimbabwe</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Country\n",
       "0                 Afghanistan\n",
       "1                     Algeria\n",
       "2                      Angola\n",
       "3                    Anguilla\n",
       "4         Antigua and Barbuda\n",
       "5                   Argentina\n",
       "6                       Aruba\n",
       "7                   Australia\n",
       "8                     Austria\n",
       "9                  Azerbaijan\n",
       "10                    Bahrain\n",
       "11                   Barbados\n",
       "12                    Belarus\n",
       "13                    Belgium\n",
       "14                     Belize\n",
       "15                    Bermuda\n",
       "16                    Bolivia\n",
       "17                     Brazil\n",
       "18     British Virgin Islands\n",
       "19                     Brunei\n",
       "20                   Bulgaria\n",
       "21               Burkina Faso\n",
       "22                    Burundi\n",
       "23                   Cameroon\n",
       "24                     Canada\n",
       "25                 Cape Verde\n",
       "26             Cayman Islands\n",
       "27                       Chad\n",
       "28                      Chile\n",
       "29                      China\n",
       "..                        ...\n",
       "135              Sint Maarten\n",
       "136                  Slovakia\n",
       "137           Solomon Islands\n",
       "138              South Africa\n",
       "139               South Korea\n",
       "140                     Spain\n",
       "141                  Suriname\n",
       "142                    Sweden\n",
       "143               Switzerland\n",
       "144                    Taiwan\n",
       "145                  Tanzania\n",
       "146                  Thailand\n",
       "147               The Bahamas\n",
       "148                The Gambia\n",
       "149                      Togo\n",
       "150       Trinidad and Tobago\n",
       "151                   Tunisia\n",
       "152                    Turkey\n",
       "153  Turks and Caicos Islands\n",
       "154                    Uganda\n",
       "155                   Ukraine\n",
       "156      United Arab Emirates\n",
       "157            United Kingdom\n",
       "158             United States\n",
       "159                   Uruguay\n",
       "160                 Venezuela\n",
       "161                   Vietnam\n",
       "162                     Yemen\n",
       "163                    Zambia\n",
       "164                  Zimbabwe\n",
       "\n",
       "[165 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(data=Table.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
